# SIRAJ Multi-Instance Architecture

## ğŸ­ True Multi-Voice AI Implementation

SIRAJ now implements **true multi-instance AI** where each teaching archetype has its own LLM state, enabling genuine multi-perspective learning.

### Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Frontend (React)                         â”‚
â”‚                  http://localhost:3000                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 SIRAJ Backend (FastAPI)                     â”‚
â”‚                  http://localhost:8000                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Multi-Instance Router (FastAPI)                â”‚
â”‚                  http://localhost:5000                      â”‚
â”‚                                                             â”‚
â”‚  â€¢ Round-robin load balancing                              â”‚
â”‚  â€¢ Archetype-to-instance mapping                           â”‚
â”‚  â€¢ Health checks & auto-restart                            â”‚
â”‚  â€¢ Request routing & failover                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                                    â”‚
           â–¼                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Gemma_Instance_A    â”‚              â”‚  Gemma_Instance_B    â”‚
â”‚  Port: 11434         â”‚              â”‚  Port: 11435         â”‚
â”‚                      â”‚              â”‚                      â”‚
â”‚  Archetypes:         â”‚              â”‚  Archetypes:         â”‚
â”‚  â€¢ Socratic          â”‚              â”‚  â€¢ Synthesizer       â”‚
â”‚  â€¢ Constructivist    â”‚              â”‚  â€¢ Challenger        â”‚
â”‚  â€¢ Storyteller       â”‚              â”‚  â€¢ Mentor            â”‚
â”‚                      â”‚              â”‚  â€¢ Analyst           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Key Features

### 1. **Dual Ollama Instances**
- **Instance A** (port 11434): Hosts questioning/building archetypes
- **Instance B** (port 11435): Hosts analytical/supportive archetypes
- Each instance maintains separate model state and memory

### 2. **Intelligent Request Router**
- Unified endpoint: `http://localhost:5000/api/query`
- Round-robin distribution for general queries
- Archetype-specific routing for council sessions
- Automatic failover if one instance becomes unhealthy

### 3. **Health Monitoring**
- Continuous health checks every 30 seconds
- Automatic restart on failure (max 3 attempts)
- Failover to healthy instance during issues
- Status dashboard at `/api/status`

### 4. **True Parallel Processing**
- Council responses generated simultaneously
- No blocking between archetype responses
- 2x faster multi-archetype generation

## ğŸ“Š Instance Mapping

| Archetype | Instance | Port | Characteristics |
|-----------|----------|------|-----------------|
| Socratic | A | 11434 | Questions, probes, explores |
| Constructivist | A | 11434 | Builds, creates, experiments |
| Storyteller | A | 11434 | Narrates, contextualizes |
| Synthesizer | B | 11435 | Integrates, unifies |
| Challenger | B | 11435 | Questions, pushes boundaries |
| Mentor | B | 11435 | Supports, encourages |
| Analyst | B | 11435 | Analyzes, quantifies |

## ğŸ”§ Configuration

### Environment Variables
```env
# Multi-instance configuration
MULTI_INSTANCE_MODE=true
ROUTER_URL=http://localhost:5000
INSTANCE_A_PORT=11434
INSTANCE_B_PORT=11435

# Model configuration
GEMMA_MODEL=gemma2:2b  # Lightweight for multi-instance
OLLAMA_NUM_PARALLEL=2   # Parallel requests per instance
```

### Starting the System
```bash
# Using the enhanced launcher
python launcher.py

# Output shows progress:
# [1/3] Starting Gemma_Instance_A...
# [2/3] Starting Gemma_Instance_B...
# [3/3] All instances ready!
```

## ğŸ“ˆ Performance Benefits

### Before (Single Instance)
- Sequential archetype responses
- Shared model state (fake personas)
- ~2-3s per archetype response
- Total council time: 14-21s for 7 archetypes

### After (Multi-Instance)
- Parallel archetype responses
- Separate model states (true multi-voice)
- ~2-3s total for all archetypes
- Total council time: 3-4s (5x faster!)

## ğŸ” Monitoring

### Router Status Endpoint
```bash
curl http://localhost:5000/api/status

{
  "instances": {
    "A": {
      "healthy": true,
      "port": 11434,
      "uptime": "0:45:23",
      "requests": 156,
      "restarts": 0
    },
    "B": {
      "healthy": true,
      "port": 11435,
      "uptime": "0:45:20",
      "requests": 148,
      "restarts": 0
    }
  },
  "round_robin_counter": 304,
  "total_requests": 304
}
```

### Health Check
```bash
curl http://localhost:5000/api/health

{
  "status": "healthy",
  "timestamp": "2025-07-26T10:30:45",
  "instances": {...}
}
```

## ğŸ› ï¸ Troubleshooting

### Instance Won't Start
```bash
# Check if ports are in use
netstat -an | grep 11434
netstat -an | grep 11435

# Kill existing Ollama processes
pkill ollama
```

### High Memory Usage
- Each instance uses ~2-4GB RAM
- Total system needs 8GB minimum
- Consider using smaller models: `gemma2:2b` instead of `gemma2:9b`

### Slow Responses
- Check instance health: `curl http://localhost:5000/api/health`
- Monitor CPU usage
- Ensure models are fully loaded
- Check network connectivity between services

## ğŸš€ Scaling Beyond Two Instances

The architecture supports easy scaling:

```python
# In launcher.py, add more instances:
INSTANCE_C_PORT = 11436
INSTANCE_D_PORT = 11437

# Update archetype mapping
ARCHETYPE_INSTANCE_MAP = {
    'socratic': 'A',
    'constructivist': 'B',
    'storyteller': 'C',
    'synthesizer': 'D',
    # ... distribute across instances
}
```

## ğŸŒŸ Benefits of Multi-Instance

1. **True Multi-Voice AI**: Each archetype has its own "brain"
2. **Faster Responses**: Parallel processing = 5x speedup
3. **Better Reliability**: Failover keeps system running
4. **Scalability**: Add more instances as needed
5. **Resource Isolation**: One crash doesn't affect others
6. **Unique Perspectives**: Real diversity in responses

## ğŸ“ Development Tips

### Testing Multi-Instance Locally
```python
# Test instance A
curl -X POST http://localhost:11434/api/generate \
  -d '{"model":"gemma2:2b","prompt":"Hello from A"}'

# Test instance B  
curl -X POST http://localhost:11435/api/generate \
  -d '{"model":"gemma2:2b","prompt":"Hello from B"}'

# Test router
curl -X POST http://localhost:5000/api/query \
  -d '{"prompt":"Test query","archetype":"socratic"}'
```

### Debugging
- Logs show which instance handles each request
- Color-coded output for easy tracking
- Health endpoints for monitoring
- Request history tracking

---

**The future of educational AI is multi-voice, multi-instance, and truly parallel!** ğŸ“ğŸ§ âœ¨
